MODEL ARCHITECTURE 1

Abbreviations{
	FC: fully connected
	CA: convolution, average pooling C1: 1D convolution
	GRU: gated recurrent unit
	ReLU: rectified linear unit
	DO: drop out
	LReLU: leaky ReLU
	CBP: convolution, batch norm., pooling
	WT: weighting
}

Fully Connected Neural Network (FCNN)
Written in: tflearn (DNN) 					L1		L2		L3		L4		L5
Loss: categorical crossentropy		Layer			FC		DO		FC		DO		FC
Learning rate: 0.0001			# units / dropout	512		20%		256		20%		2		
Optimizer: Adam				activation		ReLU				ReLU				softmax
Num Epochs: 20
	
Recurrent Neural Network (RNN)					
Written in: tflearn (DNN)					L1		L2		L3		L4		
Loss: categorical crossentropy		Layer			FC		GRU		FC		FC
Learning rate: 0.0001			# units			512		256		512		2
Optimizer: Adam				activation		ReLU				ReLU		softmax
Num Epochs: 20

RNN with Attention (RNNa)
Written in: tflearn (DNN)					L1		L2		L3		L4		L5	
Loss: categorical crossentropy		Layer			C1		GRU		WT		FC		FC
Learning rate: 0.0001			# channels / units	256		256				512		2
Optimizer: Adam				filter size		2
Num Epochs: 20				activation								ReLU		softmax

Convolutional Neural Network (CNN)
Written in: keras						L1		L2		L3		L4		L5		L6
Batch size: 48				Layer			CBP		CBP		CBP		FCB		FCB		FCB
Loss: categorical crossentropy		# channels / units	32		64		128		80		80		2
Learning rate: 0.001			filter size		16		16		16
Optimizer: Adam				activation		ReLU		ReLU		ReLU		ReLU		ReLU		softmax
Num Epochs: 40

Generative Adversarial Network and Random Forest (GAN+RF)
Written in: keras/scikit-learn		Generator			L1		L2		L3		|Critic		L1		L2		L3
Batch size: 64				Layer				FC		FC		FC				CA		FC 		FC
Learning rate generator: 0.0001		# channels / units activation	128		128		400				16(3)		128		1
Learning rate critic: 0.0002		activation 			LReLU		LReLU		tanh				ReLU		LReLU		sigmoid
Optimizer: Adam 			Random Forrest
Num Epochs: 100
Critic iter. per generator iter.: 5	Num estimators (trees): 100		Loss: categorical crossentropy 
					Max depth allowed per estimator: 45	Split criterion: Gini impurity